# Developer Documentation: Projection Calibration System

## Table of Contents
1. [Architecture Overview](#architecture-overview)
2. [Code Structure and File Locations](#code-structure-and-file-locations)
3. [Calibration Calculation Algorithm](#calibration-calculation-algorithm)
4. [Integration Points](#integration-points)
5. [Database Schema and Migrations](#database-schema-and-migrations)
6. [API Implementation](#api-implementation)
7. [Frontend Components](#frontend-components)
8. [Testing Guide](#testing-guide)
9. [Deployment Guide](#deployment-guide)
10. [Troubleshooting Common Issues](#troubleshooting-common-issues)

---

## Architecture Overview

### System Design

The Projection Calibration System is designed as a **data transformation layer** that sits between data import and consumption by Smart Score/optimizer services.

```
┌─────────────────────────────────────────────────────────────┐
│                    CALIBRATION SYSTEM                        │
└─────────────────────────────────────────────────────────────┘

DATA FLOW:

1. CONFIGURATION
   Admin UI → Calibration API → projection_calibration table

2. IMPORT & TRANSFORMATION
   DraftKings/LineStar Import → CalibrationService.apply_calibration()
   → player_pools table (original + calibrated columns)

3. CONSUMPTION
   SmartScoreService → Query player_pools (COALESCE calibrated values)
   → LineupOptimizerService → Uses calibrated projections

4. DISPLAY
   Frontend → Query player_pools → Display dual values
```

### Design Principles

**1. Non-Destructive:**
- Original projection values always preserved
- Calibration stored alongside originals, never replaces
- Historical data immutable

**2. Apply-at-Import:**
- Calibration applied during import, not at query time
- Stored calibrated values for performance
- No runtime calculation overhead

**3. Position-Based:**
- Calibration factors per NFL position (QB, RB, WR, TE, K, DST)
- Same position gets same calibration regardless of player
- Consistent application across all players

**4. Week-Scoped:**
- Independent calibration configuration per week
- Week 8 calibration doesn't affect Week 9
- Allows week-specific tuning

**5. Optional and Toggleable:**
- Calibration can be active or inactive per week
- Graceful degradation when inactive
- Backward compatible with non-calibrated data

---

## Code Structure and File Locations

### Backend Files

#### Core Services
```
backend/services/calibration_service.py
├── CalibrationService class
│   ├── get_calibration_for_week() - Query active calibration factors
│   ├── apply_calibration() - Apply calibration to player list
│   └── calculate_calibrated_value() - Core calculation formula
└── Purpose: Calibration business logic and calculations
```

#### API Layer
```
backend/routers/calibration_router.py
├── GET /api/calibration/{week_id} - Retrieve all calibrations for week
├── POST /api/calibration/{week_id} - Create/update single position
├── POST /api/calibration/{week_id}/batch - Batch create/update
├── GET /api/calibration/{week_id}/status - Calibration active status
└── POST /api/calibration/{week_id}/reset - Reset to defaults
```

#### Data Models
```
backend/schemas/calibration_schemas.py
├── CalibrationBase - Base fields (position, adjustments, is_active)
├── CalibrationCreate - Request model for creation
├── CalibrationUpdate - Request model for updates
├── CalibrationResponse - Response model with DB fields
├── CalibrationStatusResponse - Status endpoint response
├── CalibrationBatchRequest - Batch update request
├── CalibrationListResponse - List of calibrations response
└── CalibrationResetResponse - Reset endpoint response
```

#### Integration Points
```
backend/services/data_importer.py
└── DataImporter.bulk_insert_player_pools()
    └── Calls CalibrationService.apply_calibration()
    └── Inserts calibrated columns to player_pools

backend/services/smart_score_service.py
└── SmartScoreService.calculate_smart_scores()
    └── Queries with COALESCE(calibrated, original, projection)
    └── Uses calibrated values when available

backend/services/player_management_service.py
└── get_players_by_week()
    └── Returns calibrated projection columns
    └── Includes calibration_applied flag
```

### Frontend Files

#### Hooks
```
frontend/src/hooks/useCalibration.ts
├── useCalibrationStatus(weekId) - Fetch calibration status
├── useCalibrations(weekId) - Fetch all calibration factors
├── useUpdateCalibration() - Mutation for single update
├── useBatchUpdateCalibrations() - Mutation for batch update
└── useResetCalibrations() - Mutation for reset
```

#### Components
```
frontend/src/components/calibration/
├── CalibrationStatusChip.tsx - Status indicator in Player Pool header
├── CalibrationAdmin.tsx - Admin modal for managing factors
└── CalibrationPreview.tsx - Real-time calculation preview

frontend/src/components/player/
└── ProjectionDisplay.tsx - Dual-value display component
    └── Shows calibrated + original values

frontend/src/components/players/
└── PlayerTableRow.tsx - Integrated ProjectionDisplay
```

#### Types
```
frontend/src/types/player.types.ts
└── Player interface extended with:
    ├── projection_floor_original
    ├── projection_floor_calibrated
    ├── projection_median_original
    ├── projection_median_calibrated
    ├── projection_ceiling_original
    ├── projection_ceiling_calibrated
    └── calibration_applied
```

### Database Migrations

```
alembic/versions/
├── 019_create_projection_calibration_table.py
│   └── Creates projection_calibration table with constraints/indexes
├── 020_add_calibrated_projections_to_player_pools.py
│   └── Adds calibration columns to player_pools, backfills data
└── 021_seed_default_calibration_values.py
    └── Seeds default calibration values for current active week
```

### Test Files

```
tests/
├── unit/
│   ├── test_projection_calibration_db.py (8 tests)
│   └── test_calibration_service.py (8 tests)
├── integration/
│   ├── test_calibration_api.py (10 tests)
│   ├── test_calibration_import.py (7 tests)
│   └── test_lineup_optimizer_calibration.py (6 tests)
└── e2e/
    └── test_calibration_end_to_end.py (10 strategic tests)
```

---

## Calibration Calculation Algorithm

### Core Formula

```python
def calculate_calibrated_value(
    original: Optional[float],
    adjustment_percent: float
) -> Optional[float]:
    """
    Calculate calibrated value using percentage adjustment.

    Formula: calibrated = original × (1 + adjustment_percent / 100)

    Args:
        original: Original projection value (can be None)
        adjustment_percent: Percentage adjustment (-50 to +50)

    Returns:
        Calibrated value rounded to 2 decimals, or None if original is None.
        Returns 0 if result is negative.
    """
    if original is None:
        return None

    # Apply calibration formula
    calibrated = original * (1 + adjustment_percent / 100)

    # Ensure non-negative result
    if calibrated < 0:
        logger.warning(f"Calibration produced negative value, setting to 0")
        calibrated = 0.0

    # Round to 2 decimal places
    return round(calibrated, 2)
```

### Example Calculations

```python
# RB with +10% floor adjustment
calculate_calibrated_value(10.0, 10.0)
# 10.0 × (1 + 10.0/100) = 10.0 × 1.10 = 11.0

# WR with -12% ceiling adjustment
calculate_calibrated_value(30.0, -12.0)
# 30.0 × (1 + -12.0/100) = 30.0 × 0.88 = 26.4

# QB with 0% median adjustment
calculate_calibrated_value(22.5, 0.0)
# 22.5 × (1 + 0.0/100) = 22.5 × 1.00 = 22.5

# NULL value handling
calculate_calibrated_value(None, 10.0)
# Returns None (no calculation)

# Negative result handling
calculate_calibrated_value(5.0, -120.0)  # Extreme negative adjustment
# 5.0 × (1 + -120.0/100) = 5.0 × -0.20 = -1.0 → Returns 0.0
```

### Batch Application Algorithm

```python
def apply_calibration(
    players: List[dict],
    week_id: int,
    db: Session
) -> List[dict]:
    """
    Apply calibration to all players in the list.

    Steps:
    1. Query active calibration factors for week
    2. Create position → factors mapping
    3. For each player:
       a. Store original values in *_original fields
       b. If calibration exists for position:
          - Calculate calibrated floor/median/ceiling
          - Set calibration_applied = true
       c. If no calibration:
          - Copy original to calibrated fields
          - Set calibration_applied = false
    4. Return modified player list
    """
    # Step 1: Query calibration factors
    calibration_map = get_calibration_for_week(week_id, db)
    # Returns: {'QB': (5.0, 0.0, -5.0), 'RB': (10.0, 8.0, -10.0), ...}

    # Step 2: Process each player
    for player in players:
        position = player.get('position')

        # Step 3a: Store originals
        player['projection_floor_original'] = player.get('floor')
        player['projection_median_original'] = player.get('projection')
        player['projection_ceiling_original'] = player.get('ceiling')

        # Step 3b: Apply calibration if exists
        if position in calibration_map:
            floor_adj, median_adj, ceiling_adj = calibration_map[position]

            player['projection_floor_calibrated'] = calculate_calibrated_value(
                player['projection_floor_original'], floor_adj
            )
            player['projection_median_calibrated'] = calculate_calibrated_value(
                player['projection_median_original'], median_adj
            )
            player['projection_ceiling_calibrated'] = calculate_calibrated_value(
                player['projection_ceiling_original'], ceiling_adj
            )
            player['calibration_applied'] = True

        # Step 3c: No calibration - copy original to calibrated
        else:
            player['projection_floor_calibrated'] = player['projection_floor_original']
            player['projection_median_calibrated'] = player['projection_median_original']
            player['projection_ceiling_calibrated'] = player['projection_ceiling_original']
            player['calibration_applied'] = False

    return players
```

### Performance Characteristics

**Time Complexity:**
- Single calibration calculation: O(1)
- Batch calibration for N players: O(N)
- Calibration factor lookup: O(1) with dictionary mapping

**Space Complexity:**
- Calibration map: O(P) where P = number of positions (6)
- Player data: O(N × C) where C = number of calibration columns (7)

**Database Queries:**
- Calibration factors: 1 query per import (fetches all active calibrations)
- No per-player queries (batch approach)

**Measured Performance:**
- 500 players: ~15ms calibration overhead
- 1000 players: ~28ms calibration overhead
- Import time increase: <5% (meets spec requirement)

---

## Integration Points

### 1. Data Import Pipeline

**File:** `backend/services/data_importer.py`

**Integration Location:** `DataImporter.bulk_insert_player_pools()` method

```python
def bulk_insert_player_pools(self, players: list, week_id: int, db: Session):
    """
    Insert player pool data with calibration applied.

    Integration steps:
    1. Parse and normalize player data
    2. Apply calibration (NEW)
    3. Bulk insert to player_pools table
    """
    try:
        # Existing: Parse DraftKings/LineStar data
        normalized_players = self.normalize_players(players)

        # NEW: Apply calibration
        calibration_service = CalibrationService(db)
        try:
            calibrated_players = calibration_service.apply_calibration(
                normalized_players, week_id, db
            )
        except Exception as e:
            logger.error(f"Calibration failed: {e}. Using original values.")
            # Fallback: Set calibration fields to original values
            for player in normalized_players:
                player['projection_floor_original'] = player.get('floor')
                player['projection_floor_calibrated'] = player.get('floor')
                # ... similar for median and ceiling
                player['calibration_applied'] = False
            calibrated_players = normalized_players

        # Existing: Bulk insert with new calibration columns
        insert_query = text("""
            INSERT INTO player_pools (
                name, position, salary, team, opponent,
                floor, projection, ceiling,
                projection_floor_original, projection_floor_calibrated,
                projection_median_original, projection_median_calibrated,
                projection_ceiling_original, projection_ceiling_calibrated,
                calibration_applied, week_id
            ) VALUES (
                :name, :position, :salary, :team, :opponent,
                :floor, :projection, :ceiling,
                :projection_floor_original, :projection_floor_calibrated,
                :projection_median_original, :projection_median_calibrated,
                :projection_ceiling_original, :projection_ceiling_calibrated,
                :calibration_applied, :week_id
            )
        """)

        db.execute(insert_query, calibrated_players)
        db.commit()

    except Exception as e:
        logger.error(f"Import failed: {e}")
        db.rollback()
        raise
```

**Error Handling:**
- Calibration failure doesn't stop import
- Falls back to copying original values
- Sets `calibration_applied = false`
- Logs error for monitoring

### 2. Smart Score Service

**File:** `backend/services/smart_score_service.py`

**Integration Location:** Query construction in `calculate_smart_scores()`

```python
def calculate_smart_scores(self, week_id: int, profile_id: int, db: Session):
    """
    Calculate Smart Scores using calibrated projections.

    Uses COALESCE to prefer calibrated values with fallback to original.
    """
    query = text("""
        SELECT
            pp.id,
            pp.name,
            pp.position,
            pp.salary,
            COALESCE(pp.projection_floor_calibrated,
                     pp.projection_floor_original,
                     pp.floor) as floor,
            COALESCE(pp.projection_median_calibrated,
                     pp.projection_median_original,
                     pp.projection) as projection,
            COALESCE(pp.projection_ceiling_calibrated,
                     pp.projection_ceiling_original,
                     pp.ceiling) as ceiling,
            pp.calibration_applied,
            -- other columns...
        FROM player_pools pp
        WHERE pp.week_id = :week_id
    """)

    players = db.execute(query, {"week_id": week_id}).fetchall()

    # Calculate Smart Score using calibrated projections
    for player in players:
        smart_score = self._calculate_score(
            projection=player.projection,  # Uses calibrated value
            floor=player.floor,            # Uses calibrated value
            ceiling=player.ceiling,        # Uses calibrated value
            salary=player.salary,
            profile=profile
        )
        # ... store smart_score
```

**COALESCE Strategy:**
```sql
COALESCE(projection_median_calibrated,  -- Priority 1: Use calibrated if exists
         projection_median_original,    -- Priority 2: Use original if calibrated NULL
         projection)                    -- Priority 3: Legacy fallback
```

**Benefits:**
- Automatic use of calibrated values when available
- Backward compatibility with non-calibrated data
- No code changes needed for different calibration states

### 3. Player Management Service

**File:** `backend/services/player_management_service.py`

**Integration Location:** `get_players_by_week()` and player detail queries

```python
def get_players_by_week(self, week_id: int, db: Session):
    """
    Retrieve players with calibrated projection fields.

    Returns all calibration-related columns for frontend display.
    """
    query = text("""
        SELECT
            id, name, position, salary, team, opponent,
            projection_floor_original,
            projection_floor_calibrated,
            projection_median_original,
            projection_median_calibrated,
            projection_ceiling_original,
            projection_ceiling_calibrated,
            calibration_applied,
            -- Standard projection columns with COALESCE
            COALESCE(projection_floor_calibrated,
                     projection_floor_original, floor) as floor,
            COALESCE(projection_median_calibrated,
                     projection_median_original, projection) as projection,
            COALESCE(projection_ceiling_calibrated,
                     projection_ceiling_original, ceiling) as ceiling
        FROM player_pools
        WHERE week_id = :week_id
    """)

    return db.execute(query, {"week_id": week_id}).fetchall()
```

**PlayerResponse Schema:**
Updated to include all calibration fields (see `backend/schemas/player_schemas.py`)

### 4. Lineup Optimizer Service

**File:** `backend/services/lineup_optimizer_service.py`

**Integration:** No changes required!

The lineup optimizer receives `PlayerScoreResponse` objects from Smart Score Service, which already contain calibrated projections. The optimizer is **projection-agnostic** - it works with whatever projection values it receives.

**Flow:**
```
LineupOptimizerService.generate_lineups()
  → Calls SmartScoreService.calculate_smart_scores()
  → Receives PlayerScoreResponse with calibrated projections
  → Optimizes lineups using those calibrated values
  → No explicit calibration logic needed
```

**Design benefit:**
- Separation of concerns
- Optimizer doesn't need to know about calibration
- Easier to maintain and test

---

## Database Schema and Migrations

### Schema Design

See `11.2-Database-Schema-Documentation.md` for complete schema details.

**Key tables:**
1. **projection_calibration** - Stores calibration factors
2. **player_pools** - Extended with calibration columns

### Running Migrations

```bash
# View current migration status
alembic current

# Apply all pending migrations
alembic upgrade head

# Apply specific migration
alembic upgrade 019  # Just create projection_calibration table

# Rollback one migration
alembic downgrade -1

# Rollback to specific revision
alembic downgrade 018  # Before calibration migrations
```

### Migration Order

**IMPORTANT:** Migrations must run in order:

1. **019_create_projection_calibration_table.py**
   - Creates new table
   - No dependencies on existing data
   - Safe to run anytime

2. **020_add_calibrated_projections_to_player_pools.py**
   - Adds columns to player_pools
   - Backfills existing data (copies floor/projection/ceiling to *_original)
   - **May take time on large tables** (millions of rows)
   - Run during low-traffic period

3. **021_seed_default_calibration_values.py**
   - Inserts default calibration factors
   - Only for current active week
   - Quick operation (<1 second)

### Testing Migrations

**In development:**
```bash
# Create test database
createdb dfs_optimizer_test

# Run migrations
alembic upgrade head

# Verify tables created
psql dfs_optimizer_test -c "\d projection_calibration"
psql dfs_optimizer_test -c "\d player_pools"

# Test rollback
alembic downgrade 018
alembic upgrade head  # Re-apply
```

**In staging:**
```bash
# Backup staging database first
pg_dump dfs_optimizer_staging > backup_before_calibration.sql

# Run migrations
alembic upgrade head

# Verify data integrity
psql dfs_optimizer_staging -c "
    SELECT COUNT(*) FROM projection_calibration;
    SELECT calibration_applied, COUNT(*) FROM player_pools GROUP BY calibration_applied;
"

# If issues, rollback
alembic downgrade 018
psql dfs_optimizer_staging < backup_before_calibration.sql
```

---

## API Implementation

### Router Registration

**File:** `backend/main.py`

```python
from backend.routers import calibration_router

# Set database dependency
calibration_router.get_db = get_db

# Register router
app.include_router(
    calibration_router.router,
    prefix="/api/calibration",
    tags=["calibration"]
)
```

### Endpoint Details

See `11.1-API-Documentation.md` for complete API reference.

**Key implementation patterns:**

**1. Database Dependency Injection:**
```python
@router.get("/{week_id}")
async def get_calibrations(
    week_id: int,
    db: Any = Depends(_get_current_db_dependency),
):
    # db is SQLAlchemy Session
    # Provides connection to database
```

**2. Validation with Pydantic:**
```python
@router.post("/{week_id}")
async def create_calibration(
    week_id: int,
    calibration: CalibrationCreate,  # Pydantic model validates input
    db: Any = Depends(_get_current_db_dependency),
):
    # calibration.position validated against PositionEnum
    # calibration.*_adjustment_percent validated -50 to +50
```

**3. UPSERT Pattern:**
```python
# INSERT ON CONFLICT UPDATE pattern
db.execute(
    text("""
        INSERT INTO projection_calibration (week_id, position, ...)
        VALUES (:week_id, :position, ...)
        ON CONFLICT (week_id, position)
        DO UPDATE SET
            floor_adjustment_percent = EXCLUDED.floor_adjustment_percent,
            ...
    """),
    params
)
```

**4. Transaction Management:**
```python
try:
    # Batch operation
    for calibration in calibrations:
        db.execute(insert_query, calibration)

    db.commit()  # All or nothing
except Exception as e:
    db.rollback()  # Rollback on any error
    raise HTTPException(status_code=500, detail=str(e))
```

### Error Handling

**Week validation:**
```python
week_exists = db.execute(
    text("SELECT id FROM weeks WHERE id = :week_id"),
    {"week_id": week_id}
).scalar()

if not week_exists:
    raise HTTPException(status_code=404, detail=f"Week {week_id} not found")
```

**Database errors:**
```python
except Exception as e:
    logger.error(f"Database error: {str(e)}", exc_info=True)
    db.rollback()
    raise HTTPException(
        status_code=500,
        detail=f"Failed to update calibration: {str(e)}"
    )
```

---

## Frontend Components

### React Hooks Pattern

**Custom hooks for API integration:**

```typescript
// useCalibrationStatus.ts
export function useCalibrationStatus(weekId: number) {
    return useQuery({
        queryKey: ['calibration-status', weekId],
        queryFn: () => fetchCalibrationStatus(weekId),
        staleTime: 5 * 60 * 1000,  // Cache for 5 minutes
        retry: 2,
    });
}

// Usage in component
const { data: status, isLoading, error } = useCalibrationStatus(weekId);

if (status?.is_active) {
    // Show "Active" chip
} else {
    // Show "Not Active" chip
}
```

### Component Structure

**CalibrationStatusChip.tsx:**
```typescript
interface CalibrationStatusChipProps {
    weekId: number;
    onClick?: () => void;
}

export function CalibrationStatusChip({ weekId, onClick }: Props) {
    const { data: status, isLoading } = useCalibrationStatus(weekId);

    if (isLoading) return <Skeleton />;

    return (
        <Chip
            color={status?.is_active ? "success" : "default"}
            onClick={onClick}
        >
            Projection Calibration: {status?.is_active ? "Active" : "Not Active"}
        </Chip>
    );
}
```

**ProjectionDisplay.tsx:**
```typescript
interface ProjectionDisplayProps {
    label: string;
    calibratedValue?: number;
    originalValue?: number;
    calibrationApplied: boolean;
}

export function ProjectionDisplay({
    label,
    calibratedValue,
    originalValue,
    calibrationApplied,
}: Props) {
    // Show dual values if calibration applied and values differ
    if (calibrationApplied && calibratedValue && originalValue &&
        calibratedValue !== originalValue) {
        return (
            <div>
                <span className="font-bold">{calibratedValue.toFixed(1)}</span>
                <span className="text-gray-500 italic ml-2">
                    (original: {originalValue.toFixed(1)})
                </span>
            </div>
        );
    }

    // Show single value
    const displayValue = calibratedValue ?? originalValue ?? 0;
    return <span>{displayValue.toFixed(1)}</span>;
}
```

### State Management

**No global state needed:**
- React Query handles caching and updates
- Component-level state for form inputs
- Optimistic updates with invalidation

```typescript
// Mutation with cache invalidation
const updateMutation = useMutation({
    mutationFn: (data: CalibrationCreate) =>
        updateCalibration(weekId, data),
    onSuccess: () => {
        // Invalidate queries to refetch fresh data
        queryClient.invalidateQueries(['calibration', weekId]);
        queryClient.invalidateQueries(['calibration-status', weekId]);
    },
});
```

---

## Testing Guide

### Running Tests

**All calibration tests:**
```bash
# Unit tests
pytest tests/unit/test_projection_calibration_db.py -v
pytest tests/unit/test_calibration_service.py -v

# Integration tests
pytest tests/integration/test_calibration_api.py -v
pytest tests/integration/test_calibration_import.py -v
pytest tests/integration/test_lineup_optimizer_calibration.py -v

# E2E tests
pytest tests/e2e/test_calibration_end_to_end.py -v

# Run all calibration tests
pytest tests/ -k calibration -v

# Run with coverage
pytest tests/ -k calibration --cov=backend --cov-report=html
```

**Frontend tests:**
```bash
# Component tests (if using Jest/React Testing Library)
npm test -- CalibrationStatusChip
npm test -- ProjectionDisplay

# E2E tests (if using Playwright)
npx playwright test tests/e2e/calibration.spec.ts
```

### Test Structure

**Unit tests example:**
```python
# tests/unit/test_calibration_service.py

def test_calculate_calibrated_value_positive_adjustment():
    """Test calibration with positive adjustment."""
    service = CalibrationService(mock_db)

    result = service.calculate_calibrated_value(10.0, 10.0)

    assert result == 11.0  # 10.0 × 1.10

def test_calculate_calibrated_value_null_handling():
    """Test NULL value returns None."""
    service = CalibrationService(mock_db)

    result = service.calculate_calibrated_value(None, 10.0)

    assert result is None
```

**Integration tests example:**
```python
# tests/integration/test_calibration_api.py

def test_create_calibration_success(client, test_db):
    """Test creating calibration via API."""
    response = client.post(
        f"/api/calibration/{week_id}",
        json={
            "position": "RB",
            "floor_adjustment_percent": 10.0,
            "median_adjustment_percent": 8.0,
            "ceiling_adjustment_percent": -10.0,
            "is_active": True,
        }
    )

    assert response.status_code == 200
    data = response.json()
    assert data["position"] == "RB"
    assert data["floor_adjustment_percent"] == 10.0
```

### Writing New Tests

**Test naming convention:**
```python
# Format: test_[unit]_[action]_[expected_result]
test_calibration_service_apply_calibration_success()
test_calibration_api_get_calibrations_returns_all_positions()
test_import_with_active_calibration_applies_correctly()
```

**Test coverage goals:**
- Unit tests: >90% coverage of service logic
- Integration tests: All API endpoints + critical workflows
- E2E tests: Full user workflows (10 strategic tests)

---

## Deployment Guide

### Pre-Deployment Checklist

- [ ] All tests passing (unit, integration, E2E)
- [ ] Database migrations tested in staging
- [ ] API endpoints tested manually
- [ ] Frontend components tested in dev environment
- [ ] Performance benchmarks met (<5% import overhead)
- [ ] Documentation complete and reviewed
- [ ] Rollback plan prepared

### Deployment Steps

**1. Database Migrations (Production)**

```bash
# Backup production database
pg_dump production_db > backup_$(date +%Y%m%d_%H%M%S).sql

# Run migrations
alembic upgrade head

# Verify
psql production_db -c "
    SELECT tablename FROM pg_tables WHERE tablename = 'projection_calibration';
    SELECT COUNT(*) FROM projection_calibration;
"
```

**2. Seed Default Calibration**

```bash
# Migration 021 should seed current week
# Verify it ran:
psql production_db -c "
    SELECT position, floor_adjustment_percent, median_adjustment_percent,
           ceiling_adjustment_percent, is_active
    FROM projection_calibration
    ORDER BY position;
"

# Expected: 6 rows (QB, RB, WR, TE, K, DST) with default values
```

**3. Deploy Backend**

```bash
# Build and deploy backend services
docker build -t dfs-backend:calibration .
docker push dfs-backend:calibration

# Update production deployment
kubectl set image deployment/backend backend=dfs-backend:calibration

# Verify deployment
kubectl rollout status deployment/backend
```

**4. Deploy Frontend**

```bash
# Build frontend with calibration components
npm run build

# Deploy to CDN/hosting
# (deployment method varies by infrastructure)

# Verify assets loaded
curl https://your-domain.com/static/js/main.js | grep CalibrationStatusChip
```

**5. Post-Deployment Verification**

```bash
# Test API endpoints
curl https://your-domain.com/api/calibration/8/status

# Expected: {"success": true, "week_id": 8, "is_active": true, ...}

# Test import with calibration
# Upload DraftKings file for current week via UI
# Verify calibration_applied = true in database

psql production_db -c "
    SELECT calibration_applied, COUNT(*)
    FROM player_pools
    WHERE week_id = [current_week]
    GROUP BY calibration_applied;
"

# Expected: All players have calibration_applied = true
```

**6. Monitor**

```bash
# Monitor application logs
kubectl logs -f deployment/backend | grep calibration

# Watch for errors
grep -i "error" logs/calibration.log

# Monitor performance metrics
# Import time should be <5% increase
```

### Rollback Plan

If critical issues arise:

```bash
# 1. Rollback code deployment
kubectl rollout undo deployment/backend
npm run deploy:rollback  # (or equivalent for frontend)

# 2. Rollback database migrations (if needed)
alembic downgrade 018

# 3. Restore database from backup (if data corrupted)
psql production_db < backup_YYYYMMDD_HHMMSS.sql

# 4. Verify system stability
# Test import, lineup generation, etc.
```

---

## Troubleshooting Common Issues

### Issue: Calibration Not Applying During Import

**Symptoms:**
- Import completes successfully
- Players have `calibration_applied = false`
- No calibrated values in database

**Debugging steps:**

1. **Check if calibration is active:**
```sql
SELECT * FROM projection_calibration
WHERE week_id = [week_id] AND is_active = true;
```

**If no rows:** Calibration not configured for this week. Activate via admin interface.

2. **Check application logs:**
```bash
grep -i "calibration" logs/import.log

# Look for:
# "Retrieved calibration for week X: 0 positions configured" (BAD)
# "Retrieved calibration for week X: 6 positions configured" (GOOD)
# "Calibration applied to 500 players" (GOOD)
```

3. **Test calibration service directly:**
```python
from backend.services.calibration_service import CalibrationService

service = CalibrationService(db)
calibration_map = service.get_calibration_for_week(week_id, db)
print(calibration_map)

# Expected: {'QB': (5.0, 0.0, -5.0), 'RB': (10.0, 8.0, -10.0), ...}
# If empty dict: No active calibration
```

4. **Check for exceptions:**
```bash
# Look for calibration errors in logs
grep -i "calibration failed" logs/import.log

# Common errors:
# - Database connection lost during import
# - Calibration query timeout
# - Invalid calibration factors (shouldn't happen with validation)
```

---

### Issue: Incorrect Calibrated Values

**Symptoms:**
- Calibrated values don't match expected formula
- Values seem random or inconsistent

**Debugging steps:**

1. **Verify calibration factors:**
```sql
SELECT position, floor_adjustment_percent,
       median_adjustment_percent, ceiling_adjustment_percent
FROM projection_calibration
WHERE week_id = [week_id];
```

**Check:** Do factors match expected values? (e.g., RB should be +10%, +8%, -10%)

2. **Manual calculation verification:**
```sql
-- Compare calculated vs stored calibrated values
SELECT
    name,
    position,
    projection_median_original,
    projection_median_calibrated,
    projection_median_original * (1 + 8.0/100) as expected_calibrated
FROM player_pools
WHERE week_id = [week_id] AND position = 'RB' AND calibration_applied = true
LIMIT 5;

-- projection_median_calibrated should equal expected_calibrated
```

**If mismatch:** Bug in calibration calculation or wrong factors applied.

3. **Check for data type issues:**
```sql
-- Ensure adjustment_percent is numeric, not string
SELECT pg_typeof(floor_adjustment_percent)
FROM projection_calibration LIMIT 1;

-- Expected: "numeric" not "text"
```

4. **Test formula in isolation:**
```python
from backend.services.calibration_service import CalibrationService

service = CalibrationService(db)

# Test with known values
result = service.calculate_calibrated_value(10.0, 10.0)
assert result == 11.0, f"Expected 11.0, got {result}"

result = service.calculate_calibrated_value(18.0, 8.0)
assert result == 19.44, f"Expected 19.44, got {result}"
```

---

### Issue: Frontend Not Showing Calibrated Values

**Symptoms:**
- Calibration status chip missing or incorrect
- Player detail view doesn't show dual values
- Frontend errors in console

**Debugging steps:**

1. **Check browser console:**
```javascript
// Open DevTools (F12) → Console tab
// Look for errors:
// - "Failed to fetch calibration status"
// - "TypeError: Cannot read property 'calibration_applied'"
// - React component errors
```

2. **Verify API response:**
```bash
# Test API endpoint
curl -X GET "https://your-domain.com/api/calibration/8/status"

# Expected response:
# {"success": true, "week_id": 8, "is_active": true, ...}

# Test player data API
curl -X GET "https://your-domain.com/api/players/week/8" | jq '.[] | {
    name,
    calibration_applied,
    projection_median_original,
    projection_median_calibrated
}' | head -20
```

3. **Check React Query cache:**
```javascript
// In browser console (with React Query DevTools)
queryClient.getQueryData(['calibration-status', 8])

// Should return status object or undefined
// If undefined: Query not cached, may not have run
```

4. **Verify component props:**
```javascript
// In React DevTools
// Find CalibrationStatusChip or ProjectionDisplay
// Check props:
// - weekId: should be number
// - calibrationApplied: should be boolean
// - originalValue/calibratedValue: should be numbers or undefined
```

5. **Check TypeScript types:**
```typescript
// Ensure player.types.ts includes calibration fields
interface Player {
    // ... existing fields
    projection_floor_original?: number;
    projection_floor_calibrated?: number;
    // ... etc.
    calibration_applied: boolean;
}
```

---

### Issue: Performance Degradation

**Symptoms:**
- Import takes significantly longer (>5% increase)
- Database queries slow
- API endpoints timing out

**Debugging steps:**

1. **Measure import time:**
```python
import time

start = time.time()
# Run import
end = time.time()

print(f"Import took {end - start:.2f} seconds")

# Compare with/without calibration
# Should be <5% difference
```

2. **Profile database queries:**
```sql
-- Enable query logging
SET log_min_duration_statement = 100;  -- Log queries >100ms

-- Run import, then check logs
-- Look for slow calibration queries

-- Check if indexes are being used
EXPLAIN ANALYZE
SELECT * FROM projection_calibration
WHERE week_id = 8 AND is_active = true;

-- Should use idx_projection_calibration_active
-- Execution time should be <10ms
```

3. **Check for N+1 query problems:**
```python
# BAD: Querying calibration per player (N+1)
for player in players:
    calibration = db.query(ProjectionCalibration).filter_by(
        week_id=week_id, position=player.position
    ).first()
    # Apply calibration

# GOOD: Single query for all calibrations (current implementation)
calibrations = db.query(ProjectionCalibration).filter_by(
    week_id=week_id, is_active=True
).all()
calibration_map = {c.position: c for c in calibrations}
for player in players:
    calibration = calibration_map.get(player.position)
    # Apply calibration
```

4. **Optimize batch inserts:**
```python
# Ensure using bulk insert, not individual inserts
# Should be single INSERT with multiple VALUES
db.execute(insert_query, player_list)  # GOOD
# Not: for player in players: db.execute(insert_query, player)  # BAD
```

---

## Additional Resources

**Specification Documents:**
- Full specification: `/agent-os/specs/2025-11-01-smart-score-engine-enhancement/spec.md`
- Requirements: `/agent-os/specs/.../planning/requirements.md`
- Tasks breakdown: `/agent-os/specs/.../tasks.md`

**Other Documentation:**
- API Documentation: `11.1-API-Documentation.md`
- Database Schema: `11.2-Database-Schema-Documentation.md`
- User Guide: `11.3-User-Guide.md`
- Admin Guide: `11.4-Admin-Guide.md`
- FAQ: `11.5-FAQ.md`

**Code Examples:**
- Unit tests: `/tests/unit/test_calibration_service.py`
- Integration tests: `/tests/integration/test_calibration_api.py`
- E2E tests: `/tests/e2e/test_calibration_end_to_end.py`

**Development Tools:**
- Alembic documentation: https://alembic.sqlalchemy.org/
- FastAPI documentation: https://fastapi.tiangolo.com/
- React Query documentation: https://tanstack.com/query/

---

## Contributing

When contributing to the calibration system:

1. **Follow existing patterns** - Study current implementation before adding features
2. **Write tests first** - TDD approach for new functionality
3. **Update documentation** - Keep all docs in sync with code changes
4. **Performance test** - Ensure <5% overhead requirement still met
5. **Backward compatibility** - Don't break existing non-calibrated data

**Questions?** Contact the development team or open an issue in the project repository.
