# Calibration Effectiveness Metrics Tracking
# Smart Score Engine Enhancement - Projection Calibration System

**Document Version:** 1.0
**Date:** 2025-11-01
**Task:** 13.2 - Monitor Calibration Effectiveness Metrics
**Status:** Development Environment - Metrics Specification

---

## Overview

This document defines the metrics and tracking procedures for monitoring the effectiveness of the Projection Calibration System. It covers three categories of success criteria from the specification (lines 409-429):

1. **Technical Success Metrics** - System functionality and data integrity
2. **Business Success Metrics** - Lineup quality and projection accuracy improvements
3. **User Experience Metrics** - Interface clarity and usability

All metrics are designed to validate the success criteria and identify areas for optimization.

---

## 1. Technical Success Metrics

### 1.1 Calibration Application Coverage

#### Metric: Percentage of Players with Calibration Applied
**Success Criteria:** Calibration applies correctly to 100% of players during import when active (spec line 410)

**Measurement Query:**
```sql
SELECT
    w.week_number,
    COUNT(*) as total_players,
    SUM(CASE WHEN pp.calibration_applied = true THEN 1 ELSE 0 END) as calibrated_players,
    ROUND(SUM(CASE WHEN pp.calibration_applied = true THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as coverage_percent,
    pc.is_active as calibration_active
FROM player_pools pp
JOIN weeks w ON pp.week_id = w.id
LEFT JOIN (
    SELECT week_id, bool_or(is_active) as is_active
    FROM projection_calibration
    GROUP BY week_id
) pc ON w.id = pc.week_id
WHERE w.week_number >= (SELECT MAX(week_number) - 4 FROM weeks)
GROUP BY w.id, w.week_number, pc.is_active
ORDER BY w.week_number DESC;
```

**Target:** 100% when calibration is active
**Alert Threshold:**
- WARNING if coverage < 98% when calibration active
- CRITICAL if coverage < 95% when calibration active
- INFO if coverage = 0% when calibration inactive (expected)

**Tracking Frequency:** After each import
**Dashboard:** Calibration Status Dashboard (section 1.3)

**Failure Investigation Checklist:**
- Check for errors in import logs
- Verify calibration factors exist for all 6 positions
- Check for NULL original projection values
- Review transaction logs for rollbacks
- Inspect player_pools records with calibration_applied = false

---

### 1.2 Data Persistence Integrity

#### Metric: Original and Calibrated Value Preservation
**Success Criteria:** Both original and calibrated values persist correctly with no data loss (spec line 411)

**Measurement Queries:**

**Check 1: No NULL Calibrated Values When Calibration Applied**
```sql
SELECT COUNT(*) as data_loss_count
FROM player_pools
WHERE calibration_applied = true
  AND week_id = (SELECT id FROM weeks WHERE is_active = true)
  AND (
    projection_floor_calibrated IS NULL OR
    projection_median_calibrated IS NULL OR
    projection_ceiling_calibrated IS NULL
  );
```

**Check 2: Original Values Always Preserved**
```sql
SELECT COUNT(*) as missing_original_count
FROM player_pools
WHERE week_id = (SELECT id FROM weeks WHERE is_active = true)
  AND (
    projection_floor_original IS NULL OR
    projection_median_original IS NULL OR
    projection_ceiling_original IS NULL
  )
  AND (floor IS NOT NULL OR projection IS NOT NULL OR ceiling IS NOT NULL);
```

**Check 3: Calibrated Values Match Formula**
```sql
-- Sample validation: Check if calibrated values follow formula
SELECT
    pp.player_name,
    pp.position,
    pp.projection_median_original,
    pp.projection_median_calibrated,
    pc.median_adjustment_percent,
    ROUND(pp.projection_median_original * (1 + pc.median_adjustment_percent / 100.0), 2) as expected_calibrated,
    ABS(pp.projection_median_calibrated - (pp.projection_median_original * (1 + pc.median_adjustment_percent / 100.0))) as difference
FROM player_pools pp
JOIN projection_calibration pc ON pp.week_id = pc.week_id AND pp.position = pc.position
WHERE pp.calibration_applied = true
  AND pp.week_id = (SELECT id FROM weeks WHERE is_active = true)
  AND ABS(pp.projection_median_calibrated - (pp.projection_median_original * (1 + pc.median_adjustment_percent / 100.0))) > 0.01
LIMIT 10;
```

**Target:** All counts = 0
**Alert Threshold:**
- CRITICAL if data_loss_count > 0 or missing_original_count > 0
- WARNING if formula mismatch count > 5 (may indicate rounding differences)

**Tracking Frequency:** Every 15 minutes
**Dashboard:** Error Tracking Dashboard (section 3.5)

---

### 1.3 Smart Score Calibration Integration

#### Metric: Smart Score Uses Calibrated Projections
**Success Criteria:** Smart Score calculations consume calibrated values when calibration is active (spec line 412)

**Measurement Method:**

**Application Logging:**
```python
# In smart_score_service.py
logger.info(
    "smart_score_calculated",
    extra={
        "player_id": player_id,
        "week_id": week_id,
        "projection_used": projection_median_calibrated if calibration_applied else projection_median_original,
        "calibration_applied": calibration_applied,
        "smart_score": calculated_score
    }
)
```

**Validation Query:**
```sql
-- Check if Smart Score is using calibrated projections
-- This assumes smart_score is recalculated and stored
SELECT
    pp.player_name,
    pp.position,
    pp.projection_median_original,
    pp.projection_median_calibrated,
    pp.calibration_applied,
    pp.smart_score,
    -- If calibration applied, check if smart_score changed from baseline
    LAG(pp.smart_score) OVER (PARTITION BY pp.player_id ORDER BY pp.week_id) as previous_smart_score
FROM player_pools pp
WHERE pp.week_id IN (
    SELECT id FROM weeks ORDER BY week_number DESC LIMIT 2
)
ORDER BY pp.player_id, pp.week_id;
```

**Target:** 100% of Smart Score calculations use calibrated projections when calibration_applied = true
**Alert Threshold:**
- WARNING if logs show calibration_applied = true but original projection used
- CRITICAL if Smart Score calculations fail due to calibration

**Tracking Frequency:** Continuous (log monitoring)
**Dashboard:** Performance Metrics Dashboard (section 2.2)

---

### 1.4 Lineup Optimizer Integration

#### Metric: Lineup Optimizer Uses Calibrated Projections
**Success Criteria:** Lineup optimizer receives and uses calibrated projections (spec line 413)

**Measurement Method:**

**Application Logging:**
```python
# In lineup_optimizer_service.py
logger.info(
    "lineup_optimization_started",
    extra={
        "week_id": week_id,
        "player_count": len(players),
        "calibration_active": any(p.calibration_applied for p in players),
        "calibrated_player_count": sum(1 for p in players if p.calibration_applied)
    }
)
```

**Validation Query:**
```sql
-- Verify optimizer receives calibrated projections
-- Assumes lineup_player table stores the projections used
SELECT
    w.week_number,
    COUNT(DISTINCT l.id) as lineups_generated,
    AVG(lp.projection) as avg_projection_used,
    AVG(pp.projection_median_calibrated) as avg_calibrated_available,
    AVG(pp.projection_median_original) as avg_original_available,
    bool_and(pp.calibration_applied) as all_calibrated
FROM lineups l
JOIN lineup_players lp ON l.id = lp.lineup_id
JOIN player_pools pp ON lp.player_pool_id = pp.id
JOIN weeks w ON l.week_id = w.id
WHERE w.week_number >= (SELECT MAX(week_number) - 2 FROM weeks)
GROUP BY w.id, w.week_number
ORDER BY w.week_number DESC;
```

**Target:** Optimizer receives calibrated projections for 100% of players when calibration active
**Alert Threshold:**
- CRITICAL if avg_projection_used matches avg_original_available when calibration is active
- WARNING if lineups generated with mixed calibration (some players calibrated, others not)

**Tracking Frequency:** After each lineup generation
**Dashboard:** Performance Metrics Dashboard (section 2.3)

---

### 1.5 Import Performance Impact

#### Metric: Import Time Overhead
**Success Criteria:** Import process completes without performance degradation (< 5% increase in import time) (spec line 414)

**Measurement Method:**

**Application Logging:**
```python
# In data_importer.py
import_start = time.time()
# ... import process ...
calibration_start = time.time()
# ... calibration application ...
calibration_end = time.time()
import_end = time.time()

logger.info(
    "import_performance",
    extra={
        "week_id": week_id,
        "total_import_time_ms": (import_end - import_start) * 1000,
        "calibration_time_ms": (calibration_end - calibration_start) * 1000,
        "player_count": player_count,
        "calibration_overhead_percent": ((calibration_end - calibration_start) / (import_end - import_start)) * 100
    }
)
```

**Analysis Query:**
```sql
-- Calculate import time overhead
-- Assumes import_logs table with timing data
WITH baseline AS (
    SELECT AVG(total_duration_ms) as avg_baseline_time
    FROM import_logs
    WHERE calibration_applied = false
      AND created_at >= NOW() - INTERVAL '30 days'
),
calibrated AS (
    SELECT AVG(total_duration_ms) as avg_calibrated_time
    FROM import_logs
    WHERE calibration_applied = true
      AND created_at >= NOW() - INTERVAL '30 days'
)
SELECT
    b.avg_baseline_time,
    c.avg_calibrated_time,
    ROUND(((c.avg_calibrated_time - b.avg_baseline_time) / b.avg_baseline_time) * 100, 2) as overhead_percent
FROM baseline b, calibrated c;
```

**Target:** Overhead < 5%
**Alert Threshold:**
- WARNING if overhead > 5%
- CRITICAL if overhead > 10%
- INFO if individual import > 8% but average < 5% (acceptable variance)

**Tracking Frequency:** After each import
**Dashboard:** Performance Metrics Dashboard (section 2.1)

---

### 1.6 Data Integrity

#### Metric: Zero Data Corruption or Misapplication
**Success Criteria:** Zero data corruption or calibration misapplication errors (spec line 415)

**Measurement Queries:**

**Check 1: No Negative Projections**
```sql
SELECT COUNT(*) as negative_projection_count
FROM player_pools
WHERE calibration_applied = true
  AND (
    projection_floor_calibrated < 0 OR
    projection_median_calibrated < 0 OR
    projection_ceiling_calibrated < 0
  );
```

**Check 2: Logical Consistency (Floor < Median < Ceiling)**
```sql
SELECT COUNT(*) as logic_error_count
FROM player_pools
WHERE calibration_applied = true
  AND (
    projection_floor_calibrated > projection_median_calibrated OR
    projection_median_calibrated > projection_ceiling_calibrated
  );
```

**Check 3: No Calibration Applied to DraftKings Salary-Based Projections**
```sql
-- Assumes projection_source field exists
SELECT COUNT(*) as dk_calibration_error_count
FROM player_pools
WHERE calibration_applied = true
  AND projection_source = 'draftkings_salary';
```

**Check 4: Position Mismatch**
```sql
-- Verify calibration applied matches player position
SELECT COUNT(*) as position_mismatch_count
FROM player_pools pp
LEFT JOIN projection_calibration pc ON pp.week_id = pc.week_id AND pp.position = pc.position
WHERE pp.calibration_applied = true
  AND pc.id IS NULL;
```

**Target:** All counts = 0
**Alert Threshold:**
- CRITICAL if any count > 0

**Tracking Frequency:** Every 10 minutes
**Dashboard:** Error Tracking Dashboard (section 3.5)

---

### 1.7 Admin Configuration Effectiveness

#### Metric: Immediate Effect of Configuration Changes
**Success Criteria:** Admin interface allows configuration changes that take effect immediately on next import (spec line 416)

**Measurement Method:**

**Test Scenario:**
1. Admin updates calibration factor at time T
2. Import triggered at time T+1
3. Verify new calibration factor used in import

**Validation Query:**
```sql
-- Check time between calibration update and next import
WITH calibration_updates AS (
    SELECT
        week_id,
        position,
        median_adjustment_percent,
        updated_at as calibration_updated_at
    FROM projection_calibration
    WHERE updated_at >= NOW() - INTERVAL '7 days'
),
next_imports AS (
    SELECT DISTINCT ON (pp.week_id, pp.position)
        pp.week_id,
        pp.position,
        pp.projection_median_original,
        pp.projection_median_calibrated,
        pp.created_at as import_time,
        pc.median_adjustment_percent,
        ROUND(pp.projection_median_original * (1 + pc.median_adjustment_percent / 100.0), 2) as expected_value
    FROM player_pools pp
    JOIN projection_calibration pc ON pp.week_id = pc.week_id AND pp.position = pc.position
    WHERE pp.calibration_applied = true
      AND pp.created_at >= NOW() - INTERVAL '7 days'
    ORDER BY pp.week_id, pp.position, pp.created_at DESC
)
SELECT
    cu.week_id,
    cu.position,
    cu.calibration_updated_at,
    ni.import_time,
    ni.expected_value,
    ni.projection_median_calibrated,
    ABS(ni.expected_value - ni.projection_median_calibrated) as difference,
    CASE
        WHEN ABS(ni.expected_value - ni.projection_median_calibrated) < 0.01 THEN 'PASS'
        ELSE 'FAIL'
    END as test_result
FROM calibration_updates cu
JOIN next_imports ni ON cu.week_id = ni.week_id AND cu.position = ni.position
WHERE ni.import_time > cu.calibration_updated_at
ORDER BY cu.calibration_updated_at DESC;
```

**Target:** 100% of imports use latest calibration factors
**Alert Threshold:**
- WARNING if test_result = 'FAIL' for any import

**Tracking Frequency:** After each calibration update
**Dashboard:** Calibration Status Dashboard (section 1.4)

---

## 2. Business Success Metrics

### 2.1 Lineup Quality Improvement

#### Metric: Calibrated Lineup Performance vs Non-Calibrated
**Success Criteria:** Calibrated lineups score 5-10% higher on average than non-calibrated (spec line 419)

**Measurement Method:**

**Data Collection:**
```sql
-- Create tracking table for lineup actual scores
CREATE TABLE IF NOT EXISTS lineup_actual_scores (
    id SERIAL PRIMARY KEY,
    lineup_id INTEGER REFERENCES lineups(id),
    week_id INTEGER REFERENCES weeks(id),
    calibration_used BOOLEAN,
    projected_score FLOAT,
    actual_score FLOAT,  -- To be populated after games complete
    score_difference FLOAT GENERATED ALWAYS AS (actual_score - projected_score) STORED,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Analysis Query:**
```sql
-- Compare calibrated vs non-calibrated lineup performance
WITH lineup_performance AS (
    SELECT
        w.week_number,
        las.calibration_used,
        AVG(las.actual_score) as avg_actual_score,
        AVG(las.projected_score) as avg_projected_score,
        AVG(las.score_difference) as avg_difference,
        COUNT(*) as lineup_count
    FROM lineup_actual_scores las
    JOIN weeks w ON las.week_id = w.id
    WHERE las.actual_score IS NOT NULL  -- Only completed weeks
      AND w.week_number >= (SELECT MAX(week_number) - 8 FROM weeks)
    GROUP BY w.week_number, las.calibration_used
)
SELECT
    calibrated.week_number,
    calibrated.avg_actual_score as calibrated_avg_score,
    non_calibrated.avg_actual_score as non_calibrated_avg_score,
    ROUND(((calibrated.avg_actual_score - non_calibrated.avg_actual_score) / non_calibrated.avg_actual_score) * 100, 2) as improvement_percent,
    calibrated.lineup_count as calibrated_count,
    non_calibrated.lineup_count as non_calibrated_count
FROM lineup_performance calibrated
LEFT JOIN lineup_performance non_calibrated ON calibrated.week_number = non_calibrated.week_number AND non_calibrated.calibration_used = false
WHERE calibrated.calibration_used = true
ORDER BY calibrated.week_number DESC;
```

**Target:** 5-10% improvement in actual scores
**Alert Threshold:**
- SUCCESS if improvement >= 5%
- WARNING if improvement < 5% (may need calibration factor tuning)
- CRITICAL if improvement < 0% (calibration making lineups worse)

**Tracking Frequency:** Weekly (after games complete)
**Dashboard:** Create dedicated "Lineup Performance Analysis" dashboard

**Important Notes:**
- Requires 3-4 weeks of data to establish statistical significance
- Compare same contest types (e.g., GPP vs GPP, Cash vs Cash)
- Control for external factors (injuries, weather, late scratches)

---

### 2.2 Projection Range Compression

#### Metric: Floor/Ceiling Range Reduction
**Success Criteria:** Reduced projection outliers - Floor/ceiling ranges compress by 15-25% for RB/TE/WR positions (spec line 420)

**Measurement Query:**
```sql
-- Compare projection ranges calibrated vs original
WITH range_analysis AS (
    SELECT
        pp.position,
        AVG(pp.projection_ceiling_original - pp.projection_floor_original) as avg_original_range,
        AVG(pp.projection_ceiling_calibrated - pp.projection_floor_calibrated) as avg_calibrated_range,
        STDDEV(pp.projection_median_original) as original_stddev,
        STDDEV(pp.projection_median_calibrated) as calibrated_stddev
    FROM player_pools pp
    WHERE pp.calibration_applied = true
      AND pp.week_id = (SELECT id FROM weeks WHERE is_active = true)
      AND pp.position IN ('RB', 'TE', 'WR')
    GROUP BY pp.position
)
SELECT
    position,
    ROUND(avg_original_range, 2) as original_range,
    ROUND(avg_calibrated_range, 2) as calibrated_range,
    ROUND(((avg_original_range - avg_calibrated_range) / avg_original_range) * 100, 2) as compression_percent,
    ROUND(original_stddev, 2) as original_stddev,
    ROUND(calibrated_stddev, 2) as calibrated_stddev,
    CASE
        WHEN ((avg_original_range - avg_calibrated_range) / avg_original_range) * 100 BETWEEN 15 AND 25 THEN 'TARGET MET'
        WHEN ((avg_original_range - avg_calibrated_range) / avg_original_range) * 100 < 15 THEN 'UNDER TARGET'
        ELSE 'OVER TARGET'
    END as target_status
FROM range_analysis
ORDER BY position;
```

**Target:** 15-25% compression for RB, TE, WR
**Alert Threshold:**
- SUCCESS if compression between 15-25%
- WARNING if compression < 15% (may need more aggressive calibration)
- WARNING if compression > 30% (may be over-compressing, losing useful variance)

**Tracking Frequency:** Weekly
**Dashboard:** Business Metrics Dashboard (new)

**Interpretation:**
- Compression indicates more realistic projection ranges
- Too much compression can reduce useful variance for lineup optimization
- Monitor correlation with actual performance to validate effectiveness

---

### 2.3 Smart Score Distribution Quality

#### Metric: Player Separation and Distribution
**Success Criteria:** Better player pool quality - Smart Score distribution shows more separation between elite and mediocre players (spec line 421)

**Measurement Query:**
```sql
-- Analyze Smart Score distribution calibrated vs non-calibrated
WITH score_distribution AS (
    SELECT
        pp.position,
        pp.week_id,
        pp.calibration_applied,
        PERCENTILE_CONT(0.10) WITHIN GROUP (ORDER BY pp.smart_score) as p10_score,
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY pp.smart_score) as p25_score,
        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY pp.smart_score) as p50_score,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY pp.smart_score) as p75_score,
        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY pp.smart_score) as p90_score,
        STDDEV(pp.smart_score) as score_stddev,
        MAX(pp.smart_score) - MIN(pp.smart_score) as score_range
    FROM player_pools pp
    WHERE pp.week_id IN (
        SELECT id FROM weeks ORDER BY week_number DESC LIMIT 4
    )
    GROUP BY pp.position, pp.week_id, pp.calibration_applied
)
SELECT
    calibrated.position,
    ROUND(calibrated.p90_score - calibrated.p10_score, 2) as calibrated_90_10_spread,
    ROUND(non_calibrated.p90_score - non_calibrated.p10_score, 2) as non_calibrated_90_10_spread,
    ROUND(((calibrated.p90_score - calibrated.p10_score) - (non_calibrated.p90_score - non_calibrated.p10_score)) / (non_calibrated.p90_score - non_calibrated.p10_score) * 100, 2) as spread_improvement_percent,
    ROUND(calibrated.score_stddev, 2) as calibrated_stddev,
    ROUND(non_calibrated.score_stddev, 2) as non_calibrated_stddev
FROM score_distribution calibrated
LEFT JOIN score_distribution non_calibrated
    ON calibrated.position = non_calibrated.position
    AND calibrated.week_id = non_calibrated.week_id
    AND non_calibrated.calibration_applied = false
WHERE calibrated.calibration_applied = true
ORDER BY calibrated.position;
```

**Target:** Increased separation (higher 90-10 spread) indicates better differentiation
**Alert Threshold:**
- SUCCESS if spread_improvement_percent > 10%
- WARNING if spread_improvement_percent < 0% (calibration reducing differentiation)

**Tracking Frequency:** Weekly
**Dashboard:** Business Metrics Dashboard

**Interpretation:**
- Greater spread between elite (p90) and mediocre (p10) players helps optimizer select better lineups
- Too much spread may indicate over-calibration
- Monitor alongside actual lineup performance

---

### 2.4 User Adoption Rate

#### Metric: Percentage of Users with Calibration Active
**Success Criteria:** User adoption: 80%+ of users enable calibration within 2 weeks of release (spec line 422)

**Measurement Query:**
```sql
-- Track user adoption over time since feature release
WITH feature_release AS (
    SELECT MIN(created_at) as release_date
    FROM projection_calibration
),
user_activity AS (
    SELECT DISTINCT
        pp.user_id,  -- Assumes user_id tracked in imports
        MAX(pp.calibration_applied) as has_used_calibration,
        MAX(pp.created_at) as last_import_date
    FROM player_pools pp
    CROSS JOIN feature_release fr
    WHERE pp.created_at >= fr.release_date
    GROUP BY pp.user_id
)
SELECT
    COUNT(*) as total_active_users,
    SUM(CASE WHEN has_used_calibration THEN 1 ELSE 0 END) as users_with_calibration,
    ROUND(SUM(CASE WHEN has_used_calibration THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as adoption_percent,
    (SELECT ROUND(EXTRACT(EPOCH FROM (NOW() - release_date)) / 86400) FROM feature_release) as days_since_release
FROM user_activity
WHERE last_import_date >= NOW() - INTERVAL '14 days';
```

**Target:** 80% adoption within 14 days
**Alert Threshold:**
- SUCCESS if adoption >= 80% by day 14
- WARNING if adoption < 50% by day 7 (may need user education/communication)
- CRITICAL if adoption < 30% by day 14 (indicates UX or value proposition issue)

**Tracking Frequency:** Daily for first 30 days, then weekly
**Dashboard:** Usage Analytics Dashboard (section 4.5)

**Action Items if Below Target:**
- Review user feedback and support tickets
- Improve onboarding and education materials
- Consider UI/UX improvements to calibration admin
- Add in-app prompts or tooltips highlighting feature benefits

---

### 2.5 Projection Accuracy Improvement

#### Metric: RMSE Reduction for Calibrated Projections
**Success Criteria:** Projection accuracy improvement: Calibrated projections reduce RMSE (root mean square error) by 8-12% vs original projections (spec line 423)

**Measurement Method:**

**Data Collection:**
```sql
-- Track actual player scores for accuracy analysis
CREATE TABLE IF NOT EXISTS player_actual_scores (
    id SERIAL PRIMARY KEY,
    player_pool_id INTEGER REFERENCES player_pools(id),
    week_id INTEGER REFERENCES weeks(id),
    player_name VARCHAR(255),
    position VARCHAR(10),
    actual_score FLOAT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(player_pool_id, week_id)
);
```

**RMSE Calculation Query:**
```sql
-- Calculate RMSE for calibrated vs original projections
WITH projection_accuracy AS (
    SELECT
        pp.week_id,
        pp.position,
        pp.projection_median_original,
        pp.projection_median_calibrated,
        pp.calibration_applied,
        pas.actual_score,
        POWER(pp.projection_median_original - pas.actual_score, 2) as original_squared_error,
        POWER(pp.projection_median_calibrated - pas.actual_score, 2) as calibrated_squared_error
    FROM player_pools pp
    JOIN player_actual_scores pas ON pp.id = pas.player_pool_id
    WHERE pp.calibration_applied = true
      AND pas.actual_score IS NOT NULL
),
rmse_calc AS (
    SELECT
        position,
        SQRT(AVG(original_squared_error)) as original_rmse,
        SQRT(AVG(calibrated_squared_error)) as calibrated_rmse,
        COUNT(*) as sample_size
    FROM projection_accuracy
    GROUP BY position
)
SELECT
    position,
    ROUND(original_rmse, 2) as original_rmse,
    ROUND(calibrated_rmse, 2) as calibrated_rmse,
    ROUND(((original_rmse - calibrated_rmse) / original_rmse) * 100, 2) as rmse_improvement_percent,
    sample_size,
    CASE
        WHEN ((original_rmse - calibrated_rmse) / original_rmse) * 100 BETWEEN 8 AND 12 THEN 'TARGET MET'
        WHEN ((original_rmse - calibrated_rmse) / original_rmse) * 100 < 8 THEN 'UNDER TARGET'
        WHEN ((original_rmse - calibrated_rmse) / original_rmse) * 100 > 12 THEN 'EXCEEDED TARGET'
        ELSE 'NEGATIVE IMPROVEMENT'
    END as target_status
FROM rmse_calc
ORDER BY position;
```

**Target:** 8-12% RMSE reduction
**Alert Threshold:**
- SUCCESS if improvement between 8-12%
- WARNING if improvement < 5% (calibration not significantly helping)
- CRITICAL if improvement < 0% (calibration making projections worse)

**Tracking Frequency:** Weekly (after games complete)
**Dashboard:** Business Metrics Dashboard

**Important Notes:**
- Requires minimum 3-4 weeks of data for statistical significance
- Should be calculated separately for each position
- Monitor for overfitting (great historical accuracy, poor forward accuracy)
- Consider seasonal variations and opponent quality

---

## 3. User Experience Metrics

### 3.1 Calibration Status Clarity

#### Metric: User Understanding of Calibration Status
**Success Criteria:** Calibration status is clear: Users understand when calibration is active vs inactive (spec line 426)

**Measurement Methods:**

**Method 1: User Surveys**
- Post-release survey question: "Do you understand when projection calibration is active?"
- Target: 90%+ respond "Yes, very clear" or "Yes, somewhat clear"

**Method 2: Support Ticket Analysis**
```sql
-- Track support tickets related to calibration confusion
SELECT
    DATE(created_at) as date,
    COUNT(*) as tickets,
    SUM(CASE WHEN category = 'calibration_confusion' THEN 1 ELSE 0 END) as confusion_tickets,
    SUM(CASE WHEN category = 'calibration_status_unclear' THEN 1 ELSE 0 END) as status_unclear_tickets
FROM support_tickets
WHERE tags LIKE '%calibration%'
  AND created_at >= (SELECT MIN(created_at) FROM projection_calibration)
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

**Method 3: Analytics Tracking**
```javascript
// Track user interactions with status chip
analytics.track('calibration_status_chip_viewed', {
  status: 'active' | 'inactive',
  week_id: 19,
  user_action: 'none' | 'clicked' | 'hovered'
});
```

**Target:** Low support ticket volume, high survey satisfaction
**Alert Threshold:**
- WARNING if confusion_tickets > 5% of total tickets
- CRITICAL if confusion_tickets > 10% of total tickets

**Tracking Frequency:** Daily for first 30 days, then weekly
**Dashboard:** Usage Analytics Dashboard (section 4.4)

**Action Items if Below Target:**
- Enhance status chip visual design
- Add tooltips explaining calibration status
- Improve onboarding documentation
- Consider adding calibration status to more screens

---

### 3.2 Dual-Value Display Transparency

#### Metric: User Understanding of Dual-Value Display
**Success Criteria:** Dual-value display provides transparency: Users can see impact of calibration on specific players (spec line 427)

**Measurement Methods:**

**Method 1: User Engagement Tracking**
```javascript
// Track player detail views with dual values
analytics.track('player_detail_with_dual_values_viewed', {
  week_id: 19,
  player_id: '12345',
  calibration_applied: true,
  time_spent_seconds: 15
});

analytics.track('projection_value_comparison', {
  week_id: 19,
  player_id: '12345',
  original_median: 11.8,
  calibrated_median: 12.5,
  difference: 0.7,
  user_reaction: 'accepted' | 'questioned' | 'ignored'
});
```

**Method 2: A/B Test (Optional)**
- Test dual-value format variations
- Measure user comprehension and satisfaction
- Formats to test:
  - "12.5 (original: 11.8)"
  - "12.5 (was 11.8)"
  - "12.5 ↑ from 11.8"

**Method 3: User Surveys**
- Survey question: "Is it clear how calibration affects player projections?"
- Target: 85%+ respond positively

**Target:** High engagement, low confusion
**Alert Threshold:**
- WARNING if avg time_spent < 5 seconds (may not be noticed)
- WARNING if support tickets about dual-value display > 2% of total

**Tracking Frequency:** Weekly
**Dashboard:** Usage Analytics Dashboard (section 4.3)

---

### 3.3 Admin Interface Usability

#### Metric: Time to Complete Calibration Updates
**Success Criteria:** Admin interface is intuitive: Calibration factor updates completed in < 2 minutes (spec line 428)

**Measurement Method:**

**Analytics Tracking:**
```javascript
// Track admin workflow timing
analytics.track('calibration_admin_opened', {
  week_id: 19,
  timestamp: Date.now()
});

analytics.track('calibration_factor_changed', {
  week_id: 19,
  position: 'RB',
  field: 'median_adjustment',
  old_value: 5,
  new_value: 8,
  timestamp: Date.now()
});

analytics.track('calibration_admin_saved', {
  week_id: 19,
  positions_updated: ['RB', 'WR', 'TE'],
  total_time_seconds: 87,
  timestamp: Date.now()
});
```

**Analysis Query:**
```sql
-- Calculate time to complete calibration updates
WITH admin_sessions AS (
    SELECT
        session_id,
        MIN(timestamp) as session_start,
        MAX(timestamp) FILTER (WHERE event = 'calibration_admin_saved') as session_end,
        COUNT(*) FILTER (WHERE event = 'calibration_factor_changed') as changes_made
    FROM analytics_events
    WHERE event IN ('calibration_admin_opened', 'calibration_factor_changed', 'calibration_admin_saved')
      AND timestamp >= NOW() - INTERVAL '30 days'
    GROUP BY session_id
)
SELECT
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (session_end - session_start))) as median_time_seconds,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (session_end - session_start))) as p75_time_seconds,
    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (session_end - session_start))) as p90_time_seconds,
    AVG(changes_made) as avg_changes_per_session,
    COUNT(*) as total_sessions
FROM admin_sessions
WHERE session_end IS NOT NULL;
```

**Target:** Median time < 120 seconds (2 minutes)
**Alert Threshold:**
- SUCCESS if median < 120 seconds
- WARNING if median > 150 seconds
- CRITICAL if p75 > 300 seconds (indicates UX issues)

**Tracking Frequency:** Weekly
**Dashboard:** Usage Analytics Dashboard (section 4.2)

**Action Items if Below Target:**
- Simplify admin interface workflow
- Add keyboard shortcuts
- Improve input validation feedback
- Add "quick presets" for common adjustments

---

### 3.4 Projection Source Clarity

#### Metric: User Understanding of Original vs Calibrated Values
**Success Criteria:** No confusion about projection sources: Clear distinction between original and calibrated values (spec line 429)

**Measurement Methods:**

**Method 1: Support Ticket Analysis**
```sql
-- Track tickets about projection source confusion
SELECT
    COUNT(*) as projection_confusion_tickets,
    COUNT(*) FILTER (WHERE resolution_category = 'explained_original_vs_calibrated') as educational_resolutions,
    COUNT(*) FILTER (WHERE resolution_category = 'ui_bug') as ui_issues
FROM support_tickets
WHERE tags LIKE '%projection%' AND tags LIKE '%calibration%'
  AND created_at >= (SELECT MIN(created_at) FROM projection_calibration)
  AND created_at >= NOW() - INTERVAL '30 days';
```

**Method 2: User Surveys**
- Survey question: "Can you clearly distinguish between original and calibrated projection values?"
- Target: 90%+ respond "Yes, always" or "Yes, usually"

**Method 3: In-App Feedback**
```javascript
// Optional: Add feedback widget on player detail view
analytics.track('projection_display_feedback', {
  week_id: 19,
  player_id: '12345',
  clarity_rating: 1-5,  // 1 = very confusing, 5 = very clear
  comment: "Optional user comment"
});
```

**Target:** < 2% support ticket rate, > 90% survey satisfaction
**Alert Threshold:**
- WARNING if support tickets > 5% of total calibration-related tickets
- CRITICAL if survey satisfaction < 80%

**Tracking Frequency:** Weekly
**Dashboard:** Usage Analytics Dashboard

**Action Items if Below Target:**
- Enhance visual distinction (color, font weight, labeling)
- Add tooltips explaining original vs calibrated
- Improve documentation and help text
- Consider adding a legend or info icon

---

## 4. Consolidated Metrics Tracking Dashboard

### Success Criteria Summary Table

| Metric Category | Metric | Target | Current | Status | Last Updated |
|-----------------|--------|--------|---------|--------|--------------|
| **Technical** |
| Calibration Coverage | 100% when active | - | - | - |
| Data Persistence | 0 data loss errors | - | - | - |
| Smart Score Integration | 100% use calibrated | - | - | - |
| Optimizer Integration | 100% use calibrated | - | - | - |
| Import Performance | < 5% overhead | - | - | - |
| Data Integrity | 0 corruption errors | - | - | - |
| **Business** |
| Lineup Quality | 5-10% improvement | - | - | - |
| Range Compression | 15-25% for RB/TE/WR | - | - | - |
| Score Distribution | Improved separation | - | - | - |
| User Adoption | 80%+ within 2 weeks | - | - | - |
| RMSE Improvement | 8-12% reduction | - | - | - |
| **User Experience** |
| Status Clarity | 90%+ understand | - | - | - |
| Dual-Value Transparency | 85%+ clear | - | - | - |
| Admin Usability | < 120 seconds | - | - | - |
| Source Clarity | 90%+ clear | - | - | - |

### Automated Reporting

**Daily Report:**
- Technical metrics (coverage, integrity, performance)
- Error counts and trends
- User adoption progress

**Weekly Report:**
- Business metrics (lineup quality, accuracy improvement)
- User experience survey results
- Performance trends
- Action items and recommendations

**Monthly Report:**
- Executive summary of success criteria progress
- ROI analysis (time saved, lineup performance improvement)
- User feedback themes
- Recommendations for calibration factor adjustments

---

## 5. Continuous Improvement Process

### Monthly Review Checklist

1. **Technical Health Check**
   - ✅ All technical success criteria met?
   - ✅ Any data integrity issues?
   - ✅ Performance within targets?

2. **Business Value Assessment**
   - ✅ Lineup quality improving as expected?
   - ✅ Projection accuracy meeting targets?
   - ✅ User adoption on track?

3. **User Experience Validation**
   - ✅ Support ticket trends acceptable?
   - ✅ Survey satisfaction high?
   - ✅ Admin interface usable?

4. **Calibration Factor Tuning**
   - ✅ Review RMSE by position
   - ✅ Adjust factors if underperforming
   - ✅ Document changes and rationale

5. **Future Enhancement Planning**
   - ✅ Review deferred features (spec lines 379-406)
   - ✅ Prioritize based on value and effort
   - ✅ Plan Phase 2 development if successful

---

**Document Status:** Complete - Ready for production monitoring implementation
**Next Document:** 13.3-PERFORMANCE-OPTIMIZATION-PROCEDURES.md
