# Monitoring Dashboards Specification
# Smart Score Engine Enhancement - Projection Calibration System

**Document Version:** 1.0
**Date:** 2025-11-01
**Task:** 13.1 - Configure Monitoring Dashboards
**Status:** Development Environment - Dashboard Specifications

---

## Overview

This document specifies four critical monitoring dashboards for the Projection Calibration System:
1. **Calibration Status Dashboard** - Track calibration configuration and application
2. **Performance Metrics Dashboard** - Monitor system performance and efficiency
3. **Error Tracking Dashboard** - Track errors and failures
4. **Usage Analytics Dashboard** - Monitor user engagement and feature adoption

**Note:** This is a local development environment. This document provides specifications for production monitoring dashboards to be implemented in the production monitoring system (e.g., Grafana, Datadog, New Relic, or similar).

---

## Dashboard 1: Calibration Status Dashboard

### Purpose
Track calibration configuration status, application rates, and data quality across weeks and positions.

### Key Metrics

#### 1.1 Calibration Active Status by Week
**Metric:** Binary status of calibration per week
**Data Source:** `projection_calibration` table
**Query:**
```sql
SELECT
    w.week_number,
    CASE WHEN EXISTS (
        SELECT 1 FROM projection_calibration pc
        WHERE pc.week_id = w.id AND pc.is_active = true
    ) THEN 'Active' ELSE 'Inactive' END as status,
    COUNT(DISTINCT pc.position) FILTER (WHERE pc.is_active = true) as positions_configured
FROM weeks w
LEFT JOIN projection_calibration pc ON w.id = pc.week_id
GROUP BY w.id, w.week_number
ORDER BY w.week_number DESC
LIMIT 10;
```

**Visualization:** Status indicator table (last 10 weeks)
**Alert Threshold:** Warning if current week is Inactive
**Update Frequency:** Real-time on calibration updates

#### 1.2 Positions Configured per Week
**Metric:** Count of positions with calibration factors per week
**Data Source:** `projection_calibration` table
**Query:**
```sql
SELECT
    w.week_number,
    COUNT(DISTINCT pc.position) as positions_configured,
    6 as total_positions,
    ROUND(COUNT(DISTINCT pc.position) * 100.0 / 6, 1) as coverage_percent
FROM weeks w
LEFT JOIN projection_calibration pc ON w.id = pc.week_id
WHERE pc.is_active = true
GROUP BY w.id, w.week_number
ORDER BY w.week_number DESC
LIMIT 10;
```

**Visualization:** Bar chart showing coverage percentage
**Target:** 100% coverage (6/6 positions)
**Alert Threshold:** Warning if coverage < 100%
**Update Frequency:** Real-time on calibration updates

#### 1.3 Calibration Application Rate During Imports
**Metric:** Percentage of players with calibration applied per import
**Data Source:** `player_pools` table
**Query:**
```sql
SELECT
    w.week_number,
    COUNT(*) as total_players,
    SUM(CASE WHEN pp.calibration_applied = true THEN 1 ELSE 0 END) as calibrated_players,
    ROUND(SUM(CASE WHEN pp.calibration_applied = true THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as calibration_rate
FROM player_pools pp
JOIN weeks w ON pp.week_id = w.id
WHERE pp.created_at >= NOW() - INTERVAL '30 days'
GROUP BY w.id, w.week_number
ORDER BY w.week_number DESC;
```

**Visualization:** Time series line chart
**Target:** 100% when calibration is active
**Alert Threshold:** CRITICAL if rate < 95% when calibration active
**Update Frequency:** After each import

#### 1.4 Calibration Factor Distribution by Position
**Metric:** Current calibration adjustment percentages
**Data Source:** `projection_calibration` table
**Query:**
```sql
SELECT
    pc.position,
    pc.floor_adjustment_percent,
    pc.median_adjustment_percent,
    pc.ceiling_adjustment_percent,
    pc.is_active,
    pc.updated_at
FROM projection_calibration pc
JOIN weeks w ON pc.week_id = w.id
WHERE w.is_active = true
ORDER BY pc.position;
```

**Visualization:** Heatmap table showing adjustment factors
**Reference:** Compare against default values (spec lines 253-263)
**Alert Threshold:** Warning if any factor outside -50% to +50% range
**Update Frequency:** Real-time on calibration updates

#### 1.5 Data Quality Metrics
**Metric:** Calibration data integrity checks
**Data Source:** `player_pools` table
**Queries:**

**Orphaned Calibrated Values:**
```sql
SELECT COUNT(*) as orphaned_count
FROM player_pools
WHERE calibration_applied = true
  AND (
    projection_floor_calibrated IS NULL
    OR projection_median_calibrated IS NULL
    OR projection_ceiling_calibrated IS NULL
  );
```

**Missing Original Values:**
```sql
SELECT COUNT(*) as missing_original_count
FROM player_pools
WHERE calibration_applied = true
  AND (
    projection_floor_original IS NULL
    OR projection_median_original IS NULL
    OR projection_ceiling_original IS NULL
  );
```

**Negative Calibrated Values:**
```sql
SELECT COUNT(*) as negative_values_count
FROM player_pools
WHERE calibration_applied = true
  AND (
    projection_floor_calibrated < 0
    OR projection_median_calibrated < 0
    OR projection_ceiling_calibrated < 0
  );
```

**Visualization:** Single-stat panels with GREEN (0) / RED (> 0) indicators
**Target:** All counts = 0
**Alert Threshold:** CRITICAL if any count > 0
**Update Frequency:** Every 5 minutes

---

## Dashboard 2: Performance Metrics Dashboard

### Purpose
Monitor system performance, import times, query performance, and identify bottlenecks.

### Key Metrics

#### 2.1 Import Time with vs without Calibration
**Metric:** Import duration comparison
**Data Source:** Application logs (to be implemented)
**Log Format:**
```json
{
  "timestamp": "2025-11-01T10:30:00Z",
  "event": "import_complete",
  "week_id": 19,
  "duration_ms": 2450,
  "player_count": 487,
  "calibration_applied": true,
  "calibration_duration_ms": 115
}
```

**Calculations:**
- Average import time with calibration (last 10 imports)
- Average import time without calibration (historical baseline)
- Percentage overhead: `(calibrated_time - baseline_time) / baseline_time * 100`

**Visualization:** Time series line chart with dual lines
**Target:** Import overhead < 5% (spec line 414)
**Alert Threshold:** WARNING if overhead > 5%, CRITICAL if overhead > 10%
**Update Frequency:** After each import

#### 2.2 Smart Score Calculation Performance
**Metric:** Smart Score calculation duration
**Data Source:** Application logs
**Log Format:**
```json
{
  "timestamp": "2025-11-01T10:35:00Z",
  "event": "smart_score_calculation",
  "week_id": 19,
  "player_count": 487,
  "duration_ms": 234,
  "calibration_used": true
}
```

**Calculations:**
- Average calculation time per player (total_duration / player_count)
- p95 and p99 percentiles
- Comparison calibrated vs non-calibrated

**Visualization:** Histogram with percentile markers
**Target:** < 1ms per player
**Alert Threshold:** WARNING if p95 > 2ms, CRITICAL if p99 > 5ms
**Update Frequency:** Real-time during Smart Score calculations

#### 2.3 Lineup Optimizer Performance
**Metric:** Lineup generation duration with calibrated projections
**Data Source:** Application logs
**Log Format:**
```json
{
  "timestamp": "2025-11-01T10:40:00Z",
  "event": "lineup_generation",
  "week_id": 19,
  "lineup_count": 150,
  "duration_ms": 8750,
  "calibration_used": true
}
```

**Calculations:**
- Average duration per lineup
- Total generation time trends
- Comparison calibrated vs non-calibrated

**Visualization:** Box plot showing distribution
**Target:** No significant regression vs baseline
**Alert Threshold:** WARNING if 20% slower than baseline
**Update Frequency:** After each lineup generation

#### 2.4 Database Query Response Times
**Metric:** Key query performance for calibration-related operations
**Data Source:** Database slow query log + application instrumentation

**Critical Queries to Monitor:**

**Calibration Lookup Query:**
```sql
SELECT position, floor_adjustment_percent, median_adjustment_percent, ceiling_adjustment_percent
FROM projection_calibration
WHERE week_id = ? AND is_active = true;
```
**Target:** < 10ms
**Alert Threshold:** WARNING if > 50ms, CRITICAL if > 100ms

**Player Pool Query with COALESCE:**
```sql
SELECT
    COALESCE(projection_floor_calibrated, projection_floor_original, floor) as floor,
    COALESCE(projection_median_calibrated, projection_median_original, projection) as projection,
    COALESCE(projection_ceiling_calibrated, projection_ceiling_original, ceiling) as ceiling,
    calibration_applied
FROM player_pools
WHERE week_id = ?;
```
**Target:** < 50ms for 500 players
**Alert Threshold:** WARNING if > 100ms, CRITICAL if > 200ms

**Calibration Status Query:**
```sql
SELECT week_id, COUNT(*) as positions_configured
FROM projection_calibration
WHERE week_id = ? AND is_active = true
GROUP BY week_id;
```
**Target:** < 5ms
**Alert Threshold:** WARNING if > 20ms, CRITICAL if > 50ms

**Visualization:** Time series line chart (p95 response time)
**Update Frequency:** Every 5 minutes

#### 2.5 API Endpoint Response Times
**Metric:** Calibration API endpoint performance
**Data Source:** Application logs / APM tool

**Endpoints to Monitor:**
- `GET /api/calibration/{week_id}` - Target: < 50ms
- `POST /api/calibration/{week_id}` - Target: < 100ms
- `POST /api/calibration/{week_id}/batch` - Target: < 200ms
- `GET /api/calibration/{week_id}/status` - Target: < 20ms
- `POST /api/calibration/{week_id}/reset` - Target: < 150ms

**Visualization:** Multi-line time series chart
**Alert Threshold:** WARNING if p95 > 2x target, CRITICAL if p99 > 5x target
**Update Frequency:** Real-time

#### 2.6 Memory and CPU Usage
**Metric:** Resource utilization during calibration operations
**Data Source:** System monitoring (e.g., Prometheus, CloudWatch)

**Metrics:**
- CPU usage during import with calibration
- Memory usage during calibration calculation
- Database connection pool utilization

**Visualization:** Stacked area chart
**Alert Threshold:** WARNING if CPU > 80%, CRITICAL if > 95%
**Update Frequency:** Every 1 minute

---

## Dashboard 3: Error Tracking Dashboard

### Purpose
Track errors, failures, and validation issues in the calibration system.

### Key Metrics

#### 3.1 Calibration Application Errors
**Metric:** Errors during calibration application in import pipeline
**Data Source:** Application error logs
**Log Format:**
```json
{
  "timestamp": "2025-11-01T10:30:00Z",
  "level": "ERROR",
  "event": "calibration_application_failed",
  "week_id": 19,
  "position": "RB",
  "error": "Invalid adjustment factor: -75%",
  "stack_trace": "..."
}
```

**Aggregations:**
- Total errors per day
- Errors by error type
- Errors by position
- Error rate (errors / total imports)

**Visualization:** Bar chart by error type + time series
**Target:** 0 errors
**Alert Threshold:** WARNING if > 0 errors, CRITICAL if error rate > 1%
**Update Frequency:** Real-time

#### 3.2 API Endpoint Errors
**Metric:** HTTP 4xx and 5xx errors on calibration endpoints
**Data Source:** API gateway logs / application logs

**Aggregations:**
- Errors by endpoint
- Errors by status code (400, 404, 422, 500, etc.)
- Error rate per endpoint

**Critical Error Types:**
- **400 Bad Request:** Invalid calibration percentage (outside -50% to +50%)
- **404 Not Found:** Week not found
- **422 Unprocessable Entity:** Invalid position value
- **500 Internal Server Error:** Database connection issues, unexpected errors

**Visualization:** Heatmap (endpoint × status code)
**Target:** Error rate < 0.1%
**Alert Threshold:** WARNING if error rate > 1%, CRITICAL if > 5%
**Update Frequency:** Every 5 minutes

#### 3.3 Import Failures
**Metric:** Failed imports related to calibration
**Data Source:** Application logs

**Failure Scenarios:**
- Import started but calibration failed (should continue with originals)
- Transaction rollback due to calibration error
- Partial calibration application

**Query:**
```sql
-- Track imports with calibration failures
SELECT
    w.week_number,
    COUNT(*) FILTER (WHERE pp.calibration_applied = false AND pc.is_active = true) as expected_but_not_applied
FROM player_pools pp
JOIN weeks w ON pp.week_id = w.id
LEFT JOIN projection_calibration pc ON pc.week_id = w.id
GROUP BY w.id, w.week_number
HAVING COUNT(*) FILTER (WHERE pp.calibration_applied = false AND pc.is_active = true) > 0;
```

**Visualization:** Table showing failed imports
**Target:** 0 failures
**Alert Threshold:** CRITICAL if any import has calibration_applied = false when calibration is active
**Update Frequency:** After each import

#### 3.4 Validation Errors
**Metric:** Frontend and backend validation errors
**Data Source:** Application logs

**Validation Error Types:**
- Calibration percentage outside -50% to +50% range
- Invalid position value (not in QB, RB, WR, TE, K, DST)
- Missing required fields in calibration update
- Week ID not found

**Aggregations:**
- Errors by validation type
- Errors by source (frontend vs backend)
- Error trends over time

**Visualization:** Pie chart by error type
**Target:** < 5 validation errors per day (user input errors expected)
**Alert Threshold:** WARNING if > 50 per day (indicates UI issue)
**Update Frequency:** Every 15 minutes

#### 3.5 Data Integrity Violations
**Metric:** Database constraint violations and data corruption
**Data Source:** Database error logs

**Violation Types:**
- UNIQUE constraint violation (week_id + position)
- CHECK constraint violation (adjustment percentage range)
- Foreign key violation (invalid week_id)
- NULL constraint violation

**Query:**
```sql
-- Check for data integrity issues
SELECT
    'Negative calibrated values' as issue_type,
    COUNT(*) as count
FROM player_pools
WHERE calibration_applied = true
  AND (projection_floor_calibrated < 0
       OR projection_median_calibrated < 0
       OR projection_ceiling_calibrated < 0)
UNION ALL
SELECT
    'Calibrated but NULL values' as issue_type,
    COUNT(*) as count
FROM player_pools
WHERE calibration_applied = true
  AND (projection_floor_calibrated IS NULL
       OR projection_median_calibrated IS NULL
       OR projection_ceiling_calibrated IS NULL);
```

**Visualization:** Single-stat panels with 0/non-zero indicators
**Target:** 0 violations
**Alert Threshold:** CRITICAL if any count > 0
**Update Frequency:** Every 10 minutes

---

## Dashboard 4: Usage Analytics Dashboard

### Purpose
Monitor user engagement, feature adoption, and usage patterns.

### Key Metrics

#### 4.1 Admin Interface Access Frequency
**Metric:** Calibration admin modal opens
**Data Source:** Frontend analytics (e.g., Google Analytics, Mixpanel)
**Event:**
```javascript
analytics.track('calibration_admin_opened', {
  week_id: 19,
  source: 'status_chip_click' | 'settings_menu'
});
```

**Aggregations:**
- Opens per day
- Opens per week
- Unique users accessing admin interface
- Access source breakdown

**Visualization:** Time series line chart + funnel
**Target:** At least 20% of active users access admin interface
**Update Frequency:** Daily

#### 4.2 Calibration Factor Update Frequency
**Metric:** Admin calibration changes
**Data Source:** Application logs + database
**Query:**
```sql
SELECT
    DATE(pc.updated_at) as date,
    COUNT(*) as updates,
    COUNT(DISTINCT pc.week_id) as weeks_updated,
    COUNT(DISTINCT pc.position) as positions_updated
FROM projection_calibration pc
WHERE pc.updated_at >= NOW() - INTERVAL '30 days'
GROUP BY DATE(pc.updated_at)
ORDER BY date DESC;
```

**Event:**
```javascript
analytics.track('calibration_factor_updated', {
  week_id: 19,
  position: 'RB',
  floor_adjustment: 10,
  median_adjustment: 8,
  ceiling_adjustment: -10,
  action: 'manual_edit' | 'batch_update' | 'reset_defaults'
});
```

**Aggregations:**
- Updates per day
- Updates by position
- Updates by action type
- Most frequently adjusted positions

**Visualization:** Stacked bar chart by action type
**Target:** Moderate usage (not too frequent, indicates stability)
**Update Frequency:** Daily

#### 4.3 Player Detail Dual-Value View Engagement
**Metric:** User interaction with dual-value projection display
**Data Source:** Frontend analytics
**Events:**
```javascript
analytics.track('player_detail_viewed', {
  week_id: 19,
  player_id: '12345',
  calibration_applied: true
});

analytics.track('projection_value_interaction', {
  week_id: 19,
  player_id: '12345',
  value_type: 'floor' | 'median' | 'ceiling',
  has_dual_values: true
});
```

**Aggregations:**
- Player detail views with calibration active
- Player detail views without calibration active
- Percentage of views with dual-value display
- Average time spent on player details

**Visualization:** Funnel chart showing engagement
**Target:** User engagement with calibrated players
**Update Frequency:** Daily

#### 4.4 Calibration Status Chip Click Rate
**Metric:** Clicks on calibration status chip
**Data Source:** Frontend analytics
**Event:**
```javascript
analytics.track('calibration_status_chip_clicked', {
  week_id: 19,
  status: 'active' | 'inactive',
  positions_configured: 6
});
```

**Aggregations:**
- Total clicks per day
- Click-through rate (clicks / page views)
- Clicks by calibration status (active vs inactive)
- Conversion to admin modal opens

**Visualization:** Line chart + conversion funnel
**Target:** CTR > 5% indicates good discoverability
**Update Frequency:** Daily

#### 4.5 Feature Adoption Metrics
**Metric:** Overall calibration feature adoption
**Data Source:** Database + analytics

**Metrics:**
- Percentage of weeks with calibration active
- Percentage of imports using calibration
- User accounts that have adjusted calibration factors
- Weeks since last calibration adjustment per user

**Query:**
```sql
-- Calibration adoption rate
SELECT
    COUNT(DISTINCT w.id) FILTER (WHERE pc.is_active = true) as weeks_with_calibration,
    COUNT(DISTINCT w.id) as total_weeks,
    ROUND(COUNT(DISTINCT w.id) FILTER (WHERE pc.is_active = true) * 100.0 / COUNT(DISTINCT w.id), 1) as adoption_rate
FROM weeks w
LEFT JOIN projection_calibration pc ON w.id = pc.week_id
WHERE w.week_number >= (SELECT MAX(week_number) - 10 FROM weeks);
```

**Visualization:** Gauge chart showing adoption percentage
**Target:** 80% adoption within 2 weeks of release (spec line 422)
**Alert Threshold:** WARNING if adoption < 50% after 2 weeks
**Update Frequency:** Daily

#### 4.6 Calibration Reset Frequency
**Metric:** How often users reset to defaults
**Data Source:** Application logs
**Event:**
```javascript
analytics.track('calibration_reset_to_defaults', {
  week_id: 19,
  positions_reset: ['QB', 'RB', 'WR', 'TE', 'K', 'DST']
});
```

**Aggregations:**
- Reset events per day
- Resets by week
- Positions most frequently reset
- Time between calibration adjustment and reset

**Visualization:** Bar chart by position
**Target:** Low reset frequency (indicates good default values)
**Alert Threshold:** WARNING if reset rate > 20% of updates (may indicate bad defaults)
**Update Frequency:** Daily

---

## Dashboard Implementation Recommendations

### Dashboard Platform Options

1. **Grafana** (Recommended for self-hosted)
   - Pros: Open source, flexible, great database query support
   - Cons: Requires setup and maintenance
   - Best for: Full control over monitoring infrastructure

2. **Datadog**
   - Pros: Comprehensive APM, great integrations, minimal setup
   - Cons: Cost scales with usage
   - Best for: Cloud-hosted applications, quick setup

3. **New Relic**
   - Pros: Excellent APM, detailed transaction tracing
   - Cons: Pricing can be high
   - Best for: Complex applications needing deep insights

4. **CloudWatch** (if on AWS)
   - Pros: Native AWS integration, low cost
   - Cons: Limited customization vs Grafana
   - Best for: AWS-native applications

### Data Collection Strategy

#### Database Metrics
- Use database query monitoring (PostgreSQL `pg_stat_statements`)
- Set up slow query log (queries > 100ms)
- Create materialized views for complex dashboard queries
- Index strategy: Ensure all dashboard queries use existing indexes

#### Application Logging
- Structured JSON logging for all calibration events
- Log levels: INFO for operations, WARNING for degraded performance, ERROR for failures
- Include correlation IDs for tracing calibration through import → Smart Score → optimizer
- Log sampling: 100% for calibration events (low volume)

#### Frontend Analytics
- Use Google Analytics 4 or Mixpanel for user interaction tracking
- Track key events: admin_opened, calibration_updated, status_chip_clicked, player_detail_viewed
- Respect user privacy: No PII in event tracking
- Implement consent management if required

### Alert Configuration

#### Alert Channels
- **Critical Alerts:** PagerDuty, Slack (immediate notification)
- **Warning Alerts:** Email, Slack (daily digest)
- **Info Alerts:** Dashboard only (no notifications)

#### Alert Policies
- **Calibration Application Failure:** CRITICAL - page on-call engineer
- **Import Performance Degradation (>10% overhead):** CRITICAL - investigate immediately
- **Data Integrity Violation:** CRITICAL - page on-call engineer
- **API Error Rate >5%:** WARNING - investigate within 24 hours
- **Slow Query (>200ms):** WARNING - optimize within 48 hours
- **Low Adoption Rate (<50% after 2 weeks):** WARNING - review UX

### Dashboard Maintenance

#### Regular Reviews
- **Daily:** Check Error Tracking Dashboard for new issues
- **Weekly:** Review Performance Metrics Dashboard for trends
- **Monthly:** Review Usage Analytics Dashboard for adoption patterns
- **Quarterly:** Review and update alert thresholds based on baseline performance

#### Dashboard Updates
- Add new metrics as feature evolves (Phase 2: historical accuracy tracking)
- Remove deprecated metrics
- Adjust alert thresholds based on production data
- Document significant metric changes

---

## Success Criteria for Monitoring

### Technical Success
- ✅ All 4 dashboards configured and displaying real-time data
- ✅ Alert thresholds set and notifications working
- ✅ Query performance optimized (all dashboard queries < 1s)
- ✅ Data retention policy defined (30 days detailed, 1 year aggregated)

### Business Success
- ✅ Calibration effectiveness visible (lineup quality improvement tracked)
- ✅ Performance impact quantified (import time overhead measured)
- ✅ User adoption tracked and trending toward 80%+ target
- ✅ Production issues identified and resolved quickly via monitoring

### Operational Success
- ✅ On-call team has clear runbooks for alert responses
- ✅ Dashboard queries optimized to not impact production performance
- ✅ Monitoring system uptime > 99.9%
- ✅ Mean time to detection (MTTD) < 5 minutes for critical issues

---

## Next Steps

1. **Select monitoring platform** based on infrastructure and budget
2. **Implement application logging** for calibration events
3. **Configure database monitoring** for query performance
4. **Set up frontend analytics** for user engagement tracking
5. **Create dashboard configurations** (JSON/code) for automated deployment
6. **Configure alert policies** and notification channels
7. **Test monitoring system** end-to-end before production deployment
8. **Train team** on dashboard usage and alert response procedures

---

**Document Status:** Complete - Ready for production implementation
**Next Document:** 13.2-EFFECTIVENESS-METRICS-TRACKING.md
